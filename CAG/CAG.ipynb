{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPodGsLJ0JxWpOCSoKBV3w5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenBelkhair/aymenBelkhair/blob/main/CAG/CAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQtxwHIfDBkU",
        "outputId": "022f139b-f696-4dfa-c9d7-1812b36d9896"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "J6Pd9SFJ1yVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4401d10a-2bfe-4090-cc42-2bc6866b7e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain\n",
        "!pip install -q -U langchain_community\n",
        "!pip install -q -U pypdf\n",
        "!pip install -q -U langchain_google_genai\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install -q -U faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pypdf\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import faiss\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import pickle\n",
        "import os\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "ydgbvlf-JKUW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/RC-VF.pdf\"\n",
        "loader = PyPDFLoader(file_path)   #extract_images=True\n",
        "pages = []\n",
        "for page in loader.load():\n",
        "    pages.append(page)\n"
      ],
      "metadata": {
        "id": "U85RocBA27DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "id": "YHhfwlcg9GDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=''\n",
        "for i in range(len(pages)):\n",
        "    text += pages[i].page_content\n"
      ],
      "metadata": {
        "id": "SHgEGSyS518D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "4G95xezg9HaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 450, chunk_overlap = 50)\n",
        "chunks = text_splitter.split_text(text)\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWTgB6ZL6DHr",
        "outputId": "7e376b98-c2b6-46ff-ba57-5265726b7343"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GEMINI API"
      ],
      "metadata": {
        "id": "H1PdL6p-6jpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gemini-1.5-pro-latest --to know the price for every doc apploaded\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-pro-exp-02-05\",google_api_key = \"AIzaSyCiMx7mLugROXIiS0c0qkT0tgxrfz5o0aI\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key = \"AIzaSyCiMx7mLugROXIiS0c0qkT0tgxrfz5o0aI\")"
      ],
      "metadata": {
        "id": "BfP48WrV6o-B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FAISS VECT_STORE\n"
      ],
      "metadata": {
        "id": "KIkEdglo-gQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = FAISS.from_texts(chunks, embedding = embeddings)\n",
        "vector_store.save_local(\"faiss_index\")\n",
        "print(\"successful vectore store and embedding creation\")\n",
        "index = faiss.IndexFlatL2(len(embeddings.embed_query(text)))\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")\n",
        "\n",
        "documents =[page for page in pages]\n",
        "ids = [i for i in range(len(pages))]\n",
        "vector_store.add_documents(documents=documents, ids=ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Q7JFIq-lHh",
        "outputId": "e7373b0f-dd72-4b37-884e-e051b47f6fd7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "successful vectore store and embedding creation\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LRU concept"
      ],
      "metadata": {
        "id": "gAntufyDZzye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement LRU Cache Mechanism\n",
        "class LRUCache:\n",
        "    def __init__(self, capacity: int = 100):\n",
        "        self.capacity = capacity\n",
        "        self.cache = OrderedDict()\n",
        "\n",
        "    def get(self, key):\n",
        "        if key not in self.cache:\n",
        "            return None\n",
        "        # Move key to the end to indicate recent use\n",
        "        self.cache.move_to_end(key)\n",
        "        return self.cache[key]\n",
        "\n",
        "    def put(self, key, value):\n",
        "        if key in self.cache:\n",
        "            # Update the key and mark as recently used\n",
        "            self.cache.move_to_end(key)\n",
        "        self.cache[key] = value\n",
        "        # Evict the least recently used item if capacity is exceeded\n",
        "        if len(self.cache) > self.capacity:\n",
        "            self.cache.popitem(last=False)\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        return key in self.cache"
      ],
      "metadata": {
        "id": "XNFIULJJZ3If"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LRU test\n",
        "cache = LRUCache(2)\n",
        "\n",
        "cache.put(1, 1)\n",
        "print(cache.cache)\n",
        "cache.put(2, 2)\n",
        "print(cache.cache)\n",
        "cache.get(1)\n",
        "print(cache.cache)\n",
        "cache.put(3, 3)\n",
        "print(cache.cache)"
      ],
      "metadata": {
        "id": "NGrl4-FzZ-X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache.get(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-ZwUhawcOlJ",
        "outputId": "e6f9335c-0d91-483d-cd2b-ee2c02e0853b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cache creation"
      ],
      "metadata": {
        "id": "uJtgCq1XL5Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set Up the Cache Mechanism\n",
        "CACHE_FILE = \"gemini_cache.pkl\"\n",
        "CACHE_CAPACITY = 100\n",
        "\n",
        "# Try to load an existing cache, or create a new one if none exists\n",
        "if os.path.exists(CACHE_FILE):\n",
        "    with open(CACHE_FILE, \"rb\") as f:\n",
        "        cache = pickle.load(f)\n",
        "else:\n",
        "    cache = LRUCache(CACHE_CAPACITY)"
      ],
      "metadata": {
        "id": "x_-7QTgEL8-4"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Try to call gemini API"
      ],
      "metadata": {
        "id": "Gc3x5ej2JyIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inpute():\n",
        "    messages = [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"your role is to take answer the question based on the text given; and answer directly like a human using part from question. the answer is en french and if you can't find an answer reponde by 'i don't know' .\",\n",
        "        ),\n",
        "        (\"human\", \"{question} ; chercher la reponse dans ce text : \\n\\n {text}\"),\n",
        "    ]\n",
        "\n",
        "    return messages"
      ],
      "metadata": {
        "id": "ZTzQnV6GJ2uB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Q_A(question,cache):\n",
        "      cached_answer = cache.get(question)\n",
        "      if cached_answer is not None:\n",
        "          print(\"Using cached answer.\")\n",
        "          return cached_answer\n",
        "\n",
        "      else :\n",
        "              print(\"Calling Gemini API...\")\n",
        "              prompt = ChatPromptTemplate(inpute())\n",
        "              chain = prompt | llm | StrOutputParser()\n",
        "              results = vector_store.similarity_search(\n",
        "                  question,\n",
        "                  k=2\n",
        "              )\n",
        "              ai_msg = chain.invoke({\"question\":question,\"text\":results})\n",
        "\n",
        "              if ai_msg is not None:\n",
        "                # Save the answer in the LRU cache\n",
        "                cache.put(question, ai_msg)\n",
        "                # Persist the updated cache to disk\n",
        "                with open(CACHE_FILE, \"wb\") as f:\n",
        "                    pickle.dump(cache, f)\n",
        "                with open(CACHE_FILE, \"rb\") as f:\n",
        "                  cache = pickle.load(f)\n",
        "              return ai_msg"
      ],
      "metadata": {
        "id": "i3KMAUKzJ5ne"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"les visites sont-ils obligatoire\"\n",
        "\n",
        "print(Q_A(question,cache))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkqRAjykM2lQ",
        "outputId": "a915b4f8-3c6e-44a8-847b-d3c5aa161d04"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached answer.\n",
            "Les visites ne sont pas obligatoires, mais elles sont vivement conseillées.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CaMT_sHVZu1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}